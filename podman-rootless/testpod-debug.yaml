apiVersion: v1
kind: Pod
metadata:
  name: gpu-test-debug
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-test
    image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
    command: ["/bin/bash", "-c"]
    args:
    - |
      echo "=== Checking GPU devices ==="
      ls -la /dev/nvidia* 2>&1
      echo ""

      echo "=== Checking all devices ==="
      ls -la /dev/ | grep -E "(nvidia|nvidiactl|nvidia-uvm)" || echo "No NVIDIA devices found"
      echo ""

      echo "=== Checking NVIDIA libraries ==="
      ldconfig -p | grep nvidia || echo "No NVIDIA libraries in ldconfig"
      echo ""

      echo "=== Checking LD_LIBRARY_PATH ==="
      echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
      echo ""

      echo "=== Finding all CUDA/NVIDIA libraries ==="
      find /usr -name "libcuda.so*" 2>/dev/null
      find /usr -name "libnvidia*.so*" 2>/dev/null | head -10
      echo ""

      echo "=== Environment variables ==="
      env | grep -i nvidia
      echo ""

      echo "=== Trying to run nvidia-smi (if available) ==="
      which nvidia-smi && nvidia-smi || echo "nvidia-smi not found or failed"
      echo ""

      echo "=== Checking if we can load CUDA library ==="
      ldconfig -p | grep libcuda || echo "libcuda not found in library cache"
      echo ""

      echo "=== Pod resource limits ==="
      cat /proc/self/cgroup || echo "Cannot read cgroups"
      echo ""

      echo "Sleeping for 300s for manual inspection..."
      sleep 300
    resources:
      limits:
        nvidia.com/gpu: 1
